{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5109922a",
   "metadata": {},
   "source": [
    "### data source Stanford IMDB Movie review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccc94c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import os\n",
    "def get_full_data(directory, max_reviews):\n",
    "  reviews = []\n",
    "  remove_characters = \"!\\\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\" \n",
    "  punc_table = {ord(char): None for char in remove_characters}\n",
    "  \n",
    "  ctr = 1\n",
    "  for txt_file in os.listdir(directory):\n",
    "    if ctr > max_reviews: break\n",
    "    curr_file = os.path.join(directory, txt_file)\n",
    "    f = open(curr_file, \"r\", encoding=\"utf8\")  # one line\n",
    "    for line in f:\n",
    "      line = line.strip()\n",
    "      if len(line) > 0: \n",
    "        line = line.translate(punc_table) \n",
    "        line = line.lower()\n",
    "        line = \",\".join(line.split())\n",
    "        word_list = line.split(\" \") \n",
    "        reviews.append(word_list)\n",
    "    f.close()\n",
    "    ctr += 1\n",
    "  return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3ef993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_file(reviews_lists, outpt_file, w_or_a, \n",
    "  vocab_dict, max_review_len, label_char):\n",
    "\n",
    "  # write first time, append later. could use \"a+\" mode instead.\n",
    "  fout = open(outpt_file, w_or_a, encoding=\"utf8\")  \n",
    "  offset = 3  # Keras offset: 'the' = 1 (most frequent) 1+3 = 4\n",
    "      \n",
    "  for i in range(len(reviews_lists)):  # walk each review-list\n",
    "    curr_review = reviews_lists[i]\n",
    "    n_words = len(curr_review)     \n",
    "    if n_words > max_review_len:\n",
    "      continue  # next i, continue without writing anything\n",
    "\n",
    "    n_pad = max_review_len - n_words   # number 0s to prepend\n",
    "\n",
    "    for j in range(n_pad):\n",
    "      fout.write(\"0 \")\n",
    "    \n",
    "    for word in curr_review: \n",
    "      # a word in test set might not have been in train set     \n",
    "      if word not in vocab_dict:  \n",
    "        fout.write(\"2 \")   # out-of-vocab index        \n",
    "      else:\n",
    "        idx = vocab_dict[word] + offset\n",
    "        fout.write(\"%d \" % idx)\n",
    "    \n",
    "    fout.write(label_char + \"\\n\")  # '0' or '1\n",
    "        \n",
    "  fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f766cb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all():\n",
    "  print(\"Loading all reviews into memory \")\n",
    "  pos_train_reviews = get_full_data(\"data/aclImdb/train/pos\", 12500)\n",
    "  neg_train_reviews = get_full_data(\"data/aclImdb/train/neg\", 12500)\n",
    "  pos_test_reviews = get_full_data(\"data/aclImdb/test/pos\", 12500)\n",
    "  neg_test_reviews = get_full_data(\"data/aclImdb/test/neg\", 12500)\n",
    " \n",
    "\n",
    "  print(\"Analyzing reviews and making vocabulary \")\n",
    "  vocab_dict = make_vocab([pos_train_reviews, \n",
    "    neg_train_reviews])  # key = word, value = word rank\n",
    "  v_len = len(vocab_dict)  \n",
    "  # need this value, plus 4, for Embedding: 129888+4 = 129892\n",
    "  print(\"Vocab size = %d -- use this +4 for \\\n",
    "    Embedding nw \" % v_len)\n",
    "\n",
    "  max_review_len = 500  # exact fixed length\n",
    "\n",
    "  print(\"Generating training file len %d words or less \" \\\n",
    "    % max_review_len)\n",
    "\n",
    "  generate_file(pos_train_reviews, \"imdb_train_20w.txt\", \n",
    "    \"w\", vocab_dict, max_review_len, \"1\")\n",
    "  generate_file(neg_train_reviews, \"imdb_train_20w.txt\",\n",
    "    \"a\", vocab_dict, max_review_len, \"0\")\n",
    "\n",
    "  print(\"Generating test file with len %d words or less \" \\\n",
    "    % max_review_len)\n",
    "\n",
    "  generate_file(pos_test_reviews, \"imdb_test_500w.txt\", \n",
    "    \"w\", vocab_dict, max_review_len, \"1\")\n",
    "  generate_file(neg_test_reviews, \"imdb_test_500w.txt\", \n",
    "    \"a\", vocab_dict, max_review_len, \"0\")\n",
    "\n",
    "  # inspect a generated file\n",
    "  # vocab_dict was used indirectly (offset)\n",
    "\n",
    "  print(\"Displaying encoded training file: \\n\")\n",
    "  f = open(\"imdb_train_500w.txt\", \"r\", encoding=\"utf8\")\n",
    "  for line in f: \n",
    "    print(line, end=\"\")\n",
    "  f.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff45706",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  run_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f5eabf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
